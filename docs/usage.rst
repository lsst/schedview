Usage
=====

Running ``scheduler_dashboard``
-------------------------------

Begin by activating the conda environment::

    $ conda activate schedview

There are three ways to start the dashboard, depending on what you want to use
as the source of data.
One way is for users to enter arbitrary URLs or file paths from which to load
the data. **This is insecure,** because users can point the dashboard to malicious
snapshots. It is, however, much more flexible in a secure environment::

    $ scheduler_dashboard --data-from-urls

Alternately, the dashboard can be started with a flag to only allow users to
load data from a pre-specified directory on the host running the dashboard::

    $ scheduler_dashboard --data_dir /where/the/snapshot/pickles/are

In either case, the app will then give you the URL at which you can find the app.

Finally, if the dashboard is running at the USDF or another LFA facility, data can
be loaded from an S3 bucket that is already preset in the dashboard. The dashboard
will retrieve a list of snapshots for a selected night.

To start the dashbaord in LFA mode::
     $ scheduler_dashboard --lfa

Running ``prenight``
--------------------

Activate the conda environment and start the app:

::

    $ conda activate schedview
    $ prenight

The app will then give you the URL at which you can find the app.

By default, the app will allow the user to select ``opsim`` database, pickles of
scheduler instances, and rewards data from ``/sdf/group/rubin/web_data/sim-data/schedview``
(if it is being run at the USDF) or the samples directory (elsewhere).
The data directory from which a user can select files can be set on startup:

::

    $ prenight --data_dir /path/to/data/files

Alternately, ``prenight`` can be set to look at an archive of simulation
output in an S3 bucket:

::

    $ export S3_ENDPOINT_URL='https://s3dfrgw.slac.stanford.edu/'
    $ export AWS_PROFILE=prenight_aws_profile
    $ prenight --resource_uri='s3://rubin-scheduler-prenight/opsim/' --data_from_archive

where ``prenight_aws_profile`` should be replaced by whatever section of
the ``~/.lsst/aws-credentials.ini`` file has the credentials needed for
access to the ``rubin-scheduler-prenight`` bucket.

The ``resources-uri`` can also be set to a local directory tree with the same
layout as the above S3 bucket, in which case filesystem access is needed to
that directory tree, but the environment variables above are not. For example:

::

    $ prenight --resource-uri='file:///where/my/data/is/' --data_from_archive

Note that the trailing ``/`` in the ``resource-uri`` value is required.

Finally, the user can be allowed to enter arbitrary URLs for these files.
(Note that this is not secure, because it will allow the user to upload
malicious pickles. So, it should only be done when public access to the
dashboard is not possible.) Such a dashboard can be started thus:

::

    $ prenight --data_from_urls

You can also supply an initial set of data files to show on startup:

::

    $ conda activate schedview
    $ prenight --night 2023-10-01 \
    > --opsim_db /sdf/data/rubin/user/neilsen/devel/schedview/schedview/data/sample_opsim.db \
    > --scheduler /sdf/data/rubin/user/neilsen/devel/schedview/schedview/data/sample_scheduler.pickle.xz \
    > --rewards /sdf/data/rubin/user/neilsen/devel/schedview/schedview/data/sample_rewards.h5 \
    > --port 8080

The (optional) rewards data, used in the "Rewards plot" tab, can be generated
by adding an extra option to ``sim_runner`` when running the simulation that
creates the opsim database being examined.
For example, to return the data when running ``sim_runner``:

::

    >>> from rubin_sim.scheduler import sim_runner
    >>> observatory, scheduler, observations, reward_df, obs_rewards = sim_runner(
    ...     observatory,
    ...     scheduler,
    ...     sim_start_mjd=mjd_start,
    ...     sim_duration=night_duration,
    ...     record_rewards=True,
    ... )

The returned ``reward_df`` and ``obs_rewards`` data can then be saved to an `h5`
file that can then be loaded by ``prenight``:

::

    >>> rewards_fname = "my_rewards.h5"
    >>> reward_df.to_hdf(rewards_fname, "reward_df")
    >>> obs_rewards.to_hdf(rewards_fname, "obs_rewards")

To be valid, the rewards data *must* be generated by the same execution of
``sim_runner`` that generates the opsim database being examined.
